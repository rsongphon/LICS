
## PHASE 4 DOCUMENTATION: Phase 4: Data & Deployment (Weeks 11-12)

### 1. Phase Overview

This is the final "feature" phase of the LICS project, closing the loop started in Phase 3. In the previous phase, we successfully deployed and executed Python code on the edge devices. However, the data generated by those experiments (the trial results and logs) was "fire-and-forget," published into the void.

This phase is about *catching* that data. We will implement the backend ingestion pipeline to listen for MQTT messages, parse them, and store them in the database. Crucially, we will also build the real-time UI components to *visualize* this data as it arrives, providing researchers with immediate feedback. This involves creating new database tables, new API endpoints for data retrieval, and a WebSocket layer for live UI updates.

#### 1.1 Phase Objectives and Success Criteria

**Objectives:**
* Implement the backend database tables: `trial_results` and `device_logs`.
* Implement a new Alembic migration to create these tables.
* Modify the backend `core/mqtt.py` service to subscribe to the `devices/+/data` and `devices/+/logs` MQTT topics.
* Implement `on_message` handlers to parse incoming data/log JSON and save it to the new tables.
* Implement a new backend WebSocket endpoint (`/ws`) for real-time UI communication.
* Create an "MQTT-to-WebSocket" bridge: When the backend receives an MQTT message (status, data, or log), it pushes a corresponding message to all connected WebSocket clients.
* Implement new API endpoints to fetch historical data (e.g., `GET /api/deployments/{id}/results`).
* Implement a "Data Export" feature (`GET /api/deployments/{id}/export/csv`) to download trial data.
* Build a new frontend "Results" page (`/deployments/[id]`) to display deployment details and the `trial_results` table.
* Integrate WebSocket logic into the frontend to provide live updates (e.g., see new trials appear in the table without refreshing).

**Success Criteria:**
* When a PsychoPy script runs on the Pi and publishes a trial data message, that data appears in the `trial_results` table in the database *within 1 second*.
* When the Pi agent publishes a log message, it appears in the `device_logs` table.
* A user can open the `/deployments/[id]` page, and as the experiment runs, new trial rows *automatically appear* in the UI table.
* A user can open the `/deployments` list page, and the `status` of a running experiment changes from "running" to "completed" *in real-time* when the experiment finishes.
* A user can click an "Export" button on the results page and download a well-formatted CSV file of all `trial_results` for that deployment.
* All new API endpoints are protected and tested.

#### 1.2 Why this phase is sequenced at this point in the project
* **Logical Conclusion:** This phase is the entire *purpose* of the previous three.
    * Phase 1 built the tables.
    * Phase 2 built the experiment.
    * Phase 3 *ran* the experiment.
    * Phase 4 *gets the results* from the experiment.
* Without this phase, the project is a "demo." With this phase, it is a *useful scientific instrument*. It is the final piece of the core feature set.

#### 1.3 Dependencies on previous phases
* **Phase 1 (`Phase_1_Foundation.md`):** Relies on the `experiments` and `devices` tables for foreign key relationships. Relies on the `mqtt` service in `docker-compose.yml`.
* **Phase 2 (`Phase_2_PsychoPy_Builder.md`):** Relies on the `python_code` artifact. This phase will *modify* the Phase 2 compiler to include data-publishing hooks.
* **Phase 3 (`Phase_3_Edge_Integration.md`):** **Directly dependent.** This phase *listens* to the messages published by the `edge/agent.py` and the `python_code` that Phase 3 deployed. The `experiment_deployments` table is the "parent" of all `trial_results`.

#### 1.4 What must be completed before next phase can begin
* The data ingestion pipeline must be stable and tested.
* The `trial_results` and `device_logs` tables must be correctly populated.
* The core data visualization UI (results table) must be functional.
* The real-time WebSocket updates must be working reliably.
* The data export feature must be functional.
* This phase delivers the "Minimum Viable Product." The next phase (e.g., "Phase 5: Polish & Launch") would be focused on bug fixes, UI/UX improvements, documentation, and user training, rather than new features.

#### 1.5 Estimated Timeline with Buffer Considerations
* **Total:** 2 Weeks (as per Master Doc 16.4)
* **Week 11:** Backend Data Pipeline (DB Tables, MQTT Listeners, REST APIs, Export)
* **Week 12:** Frontend & Real-time (WebSocket Backend, Frontend Hook, Results UI, Live Integration)
* **Buffer:** 0.5 Weeks. The WebSocket and real-time integration can be tricky to debug. The "MQTT-to-WebSocket bridge" needs to be robust.
* **Total Allotted:** 2.5 Weeks

#### 1.6 Key deliverables and artifacts
* **Database:** New Alembic migration for `trial_results` and `device_logs`.
* **Backend Code:**
    * Modified `backend/app/core/mqtt.py` with new `on_data_message` and `on_log_message` handlers.
    * New `backend/app/api/websocket.py` (or similar) with a `ConnectionManager`.
    * New models: `backend/app/models/trial_result.py`, `.../device_log.py`.
    * New CRUD: `backend/app/crud/crud_trial_result.py`, `.../crud_device_log.py`.
    * New API routes: `backend/app/api/routes/results.py`.
    * Modified `backend/app/psychopy/compiler.py` (from Phase 2) to add data hooks.
* **Edge Code:**
    * New `edge/lics_publisher.py` (a helper library for PsychoPy to publish data).
    * Modified `edge/agent.py` to publish `stdout` to the `.../logs` topic.
* **Frontend Code:**
    * New route: `frontend/src/app/(dashboard)/deployments/[id]/page.tsx`.
    * New hook: `frontend/src/hooks/useWebSocket.ts`.
    * Modified components (`DeploymentsList.tsx`, `DeviceList.tsx`) to use the WebSocket hook.

---

### 2. Scope Definition

#### 2.1 What IS included in this phase
* **Backend - Data Ingestion:**
    * Database tables for `trial_results` and `device_logs`.
    * Alembic migration for these tables.
    * `core/mqtt.py` subscribes to `devices/+/data` and `devices/+/logs`.
    * `on_message` handler for `.../data` that parses JSON and creates a `TrialResult` record.
    * `on_message` handler for `.../logs` that parses JSON and creates a `DeviceLog` record.
* **Backend - Data Retrieval (REST):**
    * `GET /api/deployments/{id}/results`: Returns all `trial_results` for a given deployment.
    * `GET /api/devices/{id}/logs`: Returns all `device_logs` for a given device.
    * `GET /api/deployments/{id}/export/csv`: Returns a `StreamingResponse` (or file) of the trial results in CSV format.
* **Backend - Real-time (WebSocket):**
    * A WebSocket endpoint (`/ws`) that requires token authentication.
    * A `ConnectionManager` class to track active clients.
    * Modification of *all* MQTT handlers (`on_status_message` from Phase 3, plus new `on_data_message`, `on_log_message`) to broadcast messages to connected WebSocket clients.
* **Frontend - Real-time:**
    * A global `useWebSocket` hook that connects to `/ws` and provides a simple event listener interface.
    * A `WebSocketContext` provider to wrap the application.
* **Frontend - UI:**
    * A new page at `/deployments/[id]` to show deployment details and a table of `trial_results`.
    * This page will use the `useWebSocket` hook to listen for `new_trial` events and append them to the table *live*.
    * This page will have an "Export to CSV" button.
    * The existing `/deployments` page will be modified to use `useWebSocket` to listen for `deployment_update` events and update the status badges in real-time.
* **Compiler/Edge (Modifications):**
    * The `psychopy/compiler.py` (from Phase 2) will be modified to inject `import lics_publisher` and `lics_publisher.save_trial(...)` into the generated Python script.
    * A new file, `edge/lics_publisher.py`, will be created. It's a simple helper that knows how to connect to MQTT and publish a data payload. This file must be installed on the Pi by `edge/setup.sh`.
    * The `edge/agent.py` (from Phase 3) will be modified to capture `stdout` and `stderr` from the `subprocess` and publish them to the `devices/{id}/logs` topic.

#### 2.2 What IS NOT included in this phase
* **Advanced Analytics:** This phase provides the *data* and *export*. It does *not* provide any in-app charts, graphs, or statistical analysis. This is deferred to a future "Phase 5: Analytics & Polish."
* **Live Video Streaming:** While a common request, video is a completely different (and complex) technical challenge. It is out of scope.
* **Data Editing/Deletion:** `trial_results` and `device_logs` are treated as immutable, append-only logs. There will be no UI or API for "correcting" data. Data correction must happen *after* export.
* **Web-based Experiment Preview:** The `psychojs_code` column remains `null`. A live "in-browser" preview is a significant task deferred to the future.

---

### 3. Technical Architecture for This Phase

#### 3.1 Component Architecture

This phase primarily modifies the **Backend** and **Frontend**, creating a new data-flow loop.

**Data Flow: `MQTT -> Backend -> DB` (Ingestion)**

1.  **`edge/lics_publisher.py` (New):**
    * A simple Python module installed on the Pi.
    * It's initialized by the main PsychoPy script with the `deployment_id` and `device_id`.
    * `def save_trial(data: dict):`
        * `data['deployment_id'] = self.deployment_id`
        * `self.mqtt_client.publish(f"devices/{self.device_id}/data", json.dumps(data))`
2.  **`edge/agent.py` (Modified):**
    * The `subprocess.run()` call will be modified to `subprocess.Popen()` to stream `stdout` and `stderr` in real-time.
    * `for line in process.stdout:`
        * `payload = {"deployment_id": self.dep_id, "level": "INFO", "message": line}`
        * `self.mqtt_client.publish(f"devices/{self.device_id}/logs", json.dumps(payload))`
3.  **`backend/core/mqtt.py` (Modified):**
    * `__init__`: Add new subscriptions:
        * `self.client.subscribe("devices/+/data")`
        * `self.client.subscribe("devices/+/logs")`
    * `on_message`: Add new topic handlers:
        * `if topic.matches("devices/+/data"): self.on_data_message(topic, payload)`
        * `if topic.matches("devices/+/logs"): self.on_log_message(topic, payload)`
    * `on_data_message(topic, payload)`:
        * `data = json.loads(payload)`
        * `validated_data = schemas.TrialResultCreate(**data)`
        * `crud.trial_result.create(db, obj_in=validated_data)`
        * **`self.websocket_manager.broadcast({"type": "new_trial", "payload": ...})`**
    * `on_log_message(topic, payload)`:
        * `data = json.loads(payload)`
        * `data['device_id'] = topic.split('/')[1]`
        * `validated_data = schemas.DeviceLogCreate(**data)`
        * `crud.device_log.create(db, obj_in=validated_data)`
        * **`self.websocket_manager.broadcast({"type": "new_log", "payload": ...})`**

**Data Flow: `Backend -> WebSocket -> Frontend` (Real-time UI)**

1.  **`backend/api/websocket.py` (New):**
    * `class ConnectionManager:`
        * `active_connections: list[WebSocket]`
        * `async connect(websocket: WebSocket)`
        * `disconnect(websocket: WebSocket)`
        * `async broadcast(message: dict)`
    * `manager = ConnectionManager()`
    * `@router.websocket("/ws")`
        * `token = websocket.query_params.get("token")`
        * `user = await get_current_user_from_token(token)` (New auth dep)
        * `if not user: await websocket.close(); return`
        * `await manager.connect(websocket)`
        * `try: while True: await websocket.receive_text()` (Keep connection open)
        * `except WebSocketDisconnect: manager.disconnect(websocket)`
2.  **`backend/core/mqtt.py` (Modified):**
    * The service must be initialized with a reference to the `ConnectionManager`.
    * `on_status_message(topic, payload)` (from Phase 3):
        * ... (save to DB)
        * **`await self.websocket_manager.broadcast({"type": "deployment_update", "payload": ...})`**
    * `on_data_message` and `on_log_message` (see above) also call `broadcast`.
3.  **`frontend/src/hooks/useWebSocket.ts` (New):**
    * Uses `useSWR` or `react-use-websocket` library.
    * Connects to `wss://.../ws?token={auth_token}`.
    * Provides `lastMessage` and `readyState` to components.
    * Wrapped in a `WebSocketProvider` in `layout.tsx`.
4.  **`frontend/.../DeploymentsList.tsx` (Modified):**
    * `const { lastMessage } = useWebSocket()`
    * `useEffect(() => { ... })`
    * If `lastMessage.type === "deployment_update"`, use `queryClient.setQueryData()` to update the status of the specific deployment in the React Query cache, forcing a re-render.
5.  **`frontend/.../deployments/[id]/page.tsx` (New):**
    * `useQuery` to fetch initial data from `GET /api/deployments/{id}/results`.
    * `const { lastMessage } = useWebSocket()`
    * `useEffect(() => { ... })`
    * If `lastMessage.type === "new_trial"` and `payload.deployment_id === id`, append the new trial to the local state (or `queryClient.setQueryData`).

#### 3.2 Data Architecture

* **Tables to Add:** `trial_results`, `device_logs`.
* **Data Format Contract (MQTT):**
    * `devices/{device_id}/data` (Payload: `TrialResultCreate` schema)
        ```json
        {
          "deployment_id": "a1b2c3d4-...",
          "trial_number": 10,
          "started_at": "2025-11-25T10:30:01.123Z",
          "ended_at": "2025-11-25T10:30:02.456Z",
          "response": { "key_pressed": "space", "correct": true },
          "reaction_time": 345.6,
          "correct": true,
          "custom_data": { "stimulus_name": "image_01.png", "condition": "A" }
        }
        ```
    * `devices/{device_id}/logs` (Payload: `DeviceLogCreate` subset)
        ```json
        {
          "deployment_id": "a1b2c3d4-...",
          "level": "INFO",
          "message": "Experiment script started"
        }
        ```
* **Migration Scripts:**
    * `docker-compose exec backend alembic revision -m "Add trial_results and device_logs tables"`
    * See Section 7 for full DDL.

#### 3.3 Technology Stack
* **Backend (New):**
    * `fastapi.WebSocket`: Natively included in FastAPI.
    * `websockets`: Python library (often used by `uvicorn`).
    * `csv`: Standard Python library for CSV export.
* **Frontend (New):**
    * `react-use-websocket` (Recommended): A robust hook for managing WebSocket connections in React.

---

### 4. Detailed Implementation Plan

#### 4.1 Week-by-Week Breakdown

* **Week 11 Objectives: Backend Data Pipeline (DB, MQTT, REST, Export)**
    * **Tasks:**
        1.  **DB:** Define `TrialResult` and `DeviceLog` SQLAlchemy models in `backend/app/models/`.
        2.  **DB:** Define `TrialResultCreate`, `DeviceLogCreate`, etc. Pydantic schemas in `backend/app/schemas/`.
        3.  **DB:** Create `crud_trial_result.py` and `crud_device_log.py`.
        4.  **DB:** Run `alembic revision` and implement the `upgrade` function for the new tables. Apply with `alembic upgrade head`.
        5.  **MQTT:** Modify `core/mqtt.py` to subscribe to `.../data` and `.../logs`.
        6.  **MQTT:** Implement `on_data_message` and `on_log_message`. Use Pydantic to validate payloads. Wrap in `try...except` to log malformed data without crashing.
        7.  **Testing (Ingestion):** Use an MQTT client (e.g., MQTTX) to *manually* publish payloads to the topics. Verify records appear in the PostgreSQL database.
        8.  **REST API:** Create `api/routes/results.py`. Implement `GET /api/deployments/{id}/results` and `GET /api/devices/{id}/logs`.
        9.  **Export:** Implement `GET /api/deployments/{id}/export/csv`. Use `StreamingResponse` with a generator function to build the CSV row by row, preventing high memory use.
        10. **Compiler:** Modify `psychopy/compiler.py` and `.j2` templates to add `lics_publisher` hooks.
        11. **Edge:** Create and deploy `edge/lics_publisher.py` (can be part of `setup.sh`).
    * **Outputs:** A fully functional backend data pipeline. Data is being saved, and it can be retrieved via REST.
    * **Checkpoints:** Can you publish a fake trial via MQTT and see it in the DB? Can you call the `GET .../results` endpoint and see the data? Can you call the `GET .../export/csv` endpoint and get a valid file?

* **Week 12 Objectives: Frontend & Real-time (WebSocket, UI, Live Integration)**
    * **Tasks:**
        1.  **Backend (WS):** Implement `api/websocket.py` with `ConnectionManager`.
        2.  **Backend (WS):** Create the `/ws` endpoint with token auth.
        3.  **Backend (WS):** Plumb the `ConnectionManager` into `core/mqtt.py` and add `manager.broadcast` calls to *all* `on_message` handlers (status, data, logs).
        4.  **Frontend (WS):** Install `react-use-websocket`. Create `WebSocketProvider` and `useWebSocket` hook.
        5.  **Frontend (UI):** Create the new page `app/(dashboard)/deployments/[id]/page.tsx`.
        6.  **Frontend (UI):** Fetch initial data using `useQuery` and the new `.../results` REST endpoint. Display in a `<table>`.
        7.  **Frontend (Integration):** Use `useWebSocket` on the new results page. On `new_trial` messages, update the table state (or `queryClient` cache) *live*.
        8.  **Frontend (Integration):** Modify `app/(dashboard)/deployments/page.tsx` to use `useWebSocket` and update deployment statuses live.
    * **Outputs:** A functional, real-time user interface.
    * **Checkpoints:** Open the UI. Run an experiment on the Pi. Do new trials appear on the results page *without* a refresh? Does the status on the deployments list change *without* a refresh?

#### 4.2 Task Breakdown

* **Task:** Modify Edge Agent for Log Publishing
    * **Description:** Update `edge/agent.py` to capture and publish stdout/stderr from the experiment subprocess.
    * **Prerequisites:** Phase 3 `agent.py`.
    * **Implementation:**
        1.  Change `subprocess.run()` to `subprocess.Popen(..., stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)`.
        2.  Use `threading` to create two new functions: `stream_stdout(pipe)` and `stream_stderr(pipe)`.
        3.  These functions will iterate over the pipe (`for line in pipe:`).
        4.  Inside the loop, build a JSON payload: `{"deployment_id": dep_id, "level": "INFO" / "ERROR", "message": line.strip()}`.
        5.  Publish this payload to `devices/{self.device_id}/logs`.
        6.  The main `run_experiment` thread will `process.wait()` for completion, then publish the "completed" status.
    * **Acceptance Criteria:** Running an experiment (even `print("hello")`) causes a "hello" message to appear in `device_logs`.

* **Task:** Implement WebSocket Connection Manager
    * **Description:** Create the backend service for managing live WebSocket connections.
    * **Implementation:**
        1.  Create `backend/app/api/websocket.py`.
        2.  Define `class ConnectionManager`:
            * `self.active_connections: dict[str, WebSocket] = {}` (Use `user_id` or `client_id` as key).
            * `async connect(self, websocket: WebSocket, client_id: str)`: `await websocket.accept(); self.active_connections[client_id] = websocket`
            * `disconnect(self, client_id: str)`: `self.active_connections.pop(client_id, None)`
            * `async broadcast(self, message: dict)`: `payload = json.dumps(message); for conn in self.active_connections.values(): await conn.send_text(payload)`
        3.  Instantiate `manager = ConnectionManager()`.
        4.  Create the `/ws` endpoint that authenticates, gets `user.id`, and calls `manager.connect(websocket, str(user.id))`.
    * **Acceptance Criteria:** A client can connect. `manager.broadcast` sends a message to all connected clients.

---

### 5. API and Interface Specifications

* **Endpoint:** `GET /api/deployments/{id}/results` (NEW)
    * **Purpose:** Get all trial results for a single deployment, sorted by trial number.
    * **Auth:** `get_current_active_user`. (Must check user owns the experiment).
    * **Response (200):**
        ```json
        [
          {
            "id": "uuid-...",
            "deployment_id": "a1b2c3d4-...",
            "trial_number": 1,
            ... (full TrialResult schema)
          },
          {
            "id": "uuid-...",
            "deployment_id": "a1b2c3d4-...",
            "trial_number": 2,
            ...
          }
        ]
        ```

* **Endpoint:** `GET /api/devices/{id}/logs` (NEW)
    * **Purpose:** Get recent logs for a single device, sorted by time.
    * **Auth:** `get_current_active_user`. (Must check user owns the device... or experiment?).
    * **Query Params:** `?limit=100`
    * **Response (200):**
        ```json
        [
          { "id": "uuid-...", "device_id": "f1g2...", "level": "INFO", "message": "Script started", ... },
          { "id": "uuid-...", "device_id": "f1g2...", "level": "ERROR", "message": "KeyError", ... }
        ]
        ```

* **Endpoint:** `GET /api/deployments/{id}/export/csv` (NEW)
    * **Purpose:** Download trial data as a CSV.
    * **Auth:** `get_current_active_user`.
    * **Response (200):**
        * `Content-Type: text/csv`
        * `Content-Disposition: attachment; filename="deployment_{id}_results.csv"`
        * **Body (Text):**
            ```csv
            trial_number,started_at,ended_at,reaction_time,correct,response_key_pressed,custom_stimulus_name
            1,2025-11-25T10:30:01Z,2025-11-25T10:30:02Z,345.6,true,space,image_01.png
            2,2025-11-25T10:31:01Z,2025-11-25T10:31:02Z,450.1,false,q,image_02.png
            ```
        * **Note:** The backend must "flatten" the `response` and `custom_data` JSON fields into separate columns, which is a key part of this task.

* **Interface:** `WEBSOCKET /ws` (NEW)
    * **Purpose:** Real-time updates to the UI.
    * **Connection URL:** `wss://your-domain.com/api/ws?token={jwt_access_token}`
    * **Authentication:** `get_current_user_from_token(token)` dependency.
    * **Messages (Server-to-Client):**
        * `{"type": "deployment_update", "payload": { ...ExperimentDeployment schema... }}` (Sent on status change)
        * `{"type": "new_trial", "payload": { ...TrialResult schema... }}` (Sent when a new trial is ingested)
        * `{"type": "new_log", "payload": { ...DeviceLog schema... }}` (Sent when a new log is ingested)
        * `{"type": "device_update", "payload": { ...Device schema... }}` (Sent on heartbeat/status change)

---

### 6. User Interface Specifications

* **Component:** `WebSocketProvider` (in `app/(dashboard)/layout.tsx`)
    * **Purpose:** Wraps the entire dashboard to provide WebSocket context.
    * **Visual:** None.
    * **Interactions:** Fetches auth token from auth context. Establishes WebSocket connection.
    * **State:** Manages `lastMessage`, `readyState`.
    * **Integration:** Provides `useWebSocket` hook to all child components.

* **Component:** `DeploymentsList` (Modified)
    * **Purpose:** Show all deployments, now with live status.
    * **Interactions:**
        * `const { lastMessage } = useWebSocket()`
        * `useEffect` listens to `lastMessage`.
        * If `lastMessage.type === "deployment_update"`, it finds the matching deployment in the `react-query` cache and updates its `status`.
    * **Visual:** The "Status" badge for a deployment will change from "running" to "completed" automatically.

* **Component:** `DeploymentResultsPage` (`app/(dashboard)/deployments/[id]/page.tsx`)
    * **Purpose:** Show details and all trial results for one deployment.
    * **Visual:**
        * Header: "Deployment: {exp_name} on {device_name}"
        * Summary component: Shows status, start time, end time.
        * "Export to CSV" button.
        * `<table>` for `trial_results`. Columns: `Trial #`, `Start Time`, `RT (ms)`, `Correct`, `Response` (as string), `Custom Data` (as string).
    * **State:** `useQuery` to fetch initial results. `useState` to hold the list of trials.
    * **Interactions:**
        * `useWebSocket` listens for `new_trial` messages.
        * When a `new_trial` message arrives *with a matching `deployment_id`*, it is appended to the local state, causing the table to re-render and show the new row.
        * "Export" button click triggers a download to the `/export/csv` endpoint.

---

### 7. Database Implementation

#### 7.1 Schema Design
* **`app/models/trial_result.py`** (NEW)
    ```python
    import uuid
    from datetime import datetime
    from sqlalchemy import Column, Integer, Boolean, DateTime, Float, ForeignKey
    from sqlalchemy.dialects.postgresql import UUID, JSONB
    from app.db.base_class import Base

    class TrialResult(Base):
        id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
        
        deployment_id = Column(UUID(as_uuid=True), ForeignKey("experiment_deployments.id", ondelete="CASCADE"), nullable=False, index=True)
        
        trial_number = Column(Integer, nullable=False)
        started_at = Column(DateTime(timezone=True), nullable=False)
        ended_at = Column(DateTime(timezone=True), nullable=False)
        
        response = Column(JSONB)  # e.g., {"key_pressed": "space"}
        reaction_time = Column(Float)  # in milliseconds
        correct = Column(Boolean)
        
        custom_data = Column(JSONB) # e.g., {"stimulus_name": "img.png"}
    ```

* **`app/models/device_log.py`** (NEW)
    ```python
    import uuid
    import enum
    from datetime import datetime
    from sqlalchemy import Column, String, DateTime, ForeignKey, Enum, Text
    from sqlalchemy.dialects.postgresql import UUID, JSONB
    from app.db.base_class import Base

    class LogLevel(str, enum.Enum):
        DEBUG = "DEBUG"
        INFO = "INFO"
        WARNING = "WARNING"
        ERROR = "ERROR"

    class DeviceLog(Base):
        id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
        
        device_id = Column(UUID(as_uuid=True), ForeignKey("devices.id", ondelete="CASCADE"), nullable=False, index=True)
        
        # Optional, to link a log message to a specific run
        deployment_id = Column(UUID(as_uuid=True), ForeignKey("experiment_deployments.id", ondelete="SET NULL"), nullable=True, index=True) 

        level = Column(Enum(LogLevel), default=LogLevel.INFO, nullable=False)
        message = Column(Text, nullable=False)
        
        created_at = Column(DateTime(timezone=True), default=datetime.utcnow, nullable=False, index=True)
    ```

#### 7.2 Migration Strategy
1.  Run `docker-compose exec backend alembic revision -m "Add trial_results and device_logs tables"`
2.  Add the SQLAlchemy models to `app/db/base.py` to be visible to Alembic.
3.  Alembic `autogenerate` should pick up the new models. Add the DDL to the migration file:
    ```python
    # In migration file:
    log_level_enum = sa.Enum('DEBUG', 'INFO', 'WARNING', 'ERROR', name='loglevel')
    log_level_enum.create(op.get_bind(), checkfirst=True)
    
    op.create_table('device_logs',
        sa.Column('id', sa.UUID(), ...),
        sa.Column('device_id', sa.UUID(), ...),
        sa.Column('deployment_id', sa.UUID(), ...),
        sa.Column('level', log_level_enum, ...),
        ...
        sa.ForeignKeyConstraint(['device_id'], ['devices.id'], ondelete='CASCADE'),
        ...
    )
    
    op.create_table('trial_results',
        sa.Column('id', sa.UUID(), ...),
        sa.Column('deployment_id', sa.UUID(), ...),
        ...
        sa.ForeignKeyConstraint(['deployment_id'], ['experiment_deployments.id'], ondelete='CASCADE'),
        ...
    )
    ```
4.  Run `docker-compose exec backend alembic upgrade head` to apply.

---

### 8. Testing Strategy for This Phase

#### 8.1 Unit Testing
* **Target (Backend):** `core/mqtt.py` message handlers.
    * `test_on_data_message`:
        * Scenario: Valid JSON payload. (Assert `crud.trial_result.create` is called).
        * Scenario: Malformed JSON. (Assert `json.JSONDecodeError` is caught and logged).
        * Scenario: Missing required field (e.g., `deployment_id`). (Assert `pydantic.ValidationError` is caught and logged).
* **Target (Backend):** `api/routes/results.py` (Export)
    * `test_export_csv`: Mock `crud.trial_result.get_multi` to return 3 results. Call the endpoint. Assert the `StreamingResponse` content is a valid 4-line CSV.
    * `test_export_csv_flattens_json`: Assert that `{"custom_data": {"a": 1}}` becomes a column named `custom_data_a`.

#### 8.2 Integration Testing
* **Target:** `MQTT -> DB` pipeline.
    * **Strategy:** Use `pytest` with the `client` and `db` fixtures. In a test, *manually* publish a message to the MQTT broker. `time.sleep(1)`. Query the `db` directly to assert the `TrialResult` was created.
* **Target:** `MQTT -> WebSocket` pipeline.
    * **Strategy:** This is the most complex.
        1.  In `pytest`, create a `WebSocketTestClient` for the `/ws` endpoint (with a valid token).
        2.  Manually publish a message to the `devices/+/data` topic.
        3.  `time.sleep(1)`.
        4.  Call `client.receive_json()` on the WebSocket client.
        5.  Assert the received JSON matches `{"type": "new_trial", ...}`.

#### 8.3 End-to-End Testing
* **Scenario:** "The Live Experiment"
    1.  User logs in.
    2.  User navigates to the builder, compiles an experiment.
    3.  User clicks "Deploy," selects an online Pi.
    4.  User navigates to the `/deployments` list. The status shows "running" (this may update live from "pending").
    5.  User clicks the new deployment, navigating to `/deployments/[id]`.
    6.  The page is empty. As the experiment runs on the Pi, new trial rows appear in the table *one by one* without the user refreshing.
    7.  The Pi's `agent.py` log shows `stdout` messages being published.
    8.  The user clicks "Export to CSV" and receives a file containing all trials.
    9.  The user navigates back to `/deployments`. The status now shows "completed" (this should have updated live).

---

### 9. Security Implementation

* **WebSocket Authentication (CRITICAL):**
    * The `/ws` endpoint *cannot* use standard `Authorization: Bearer` headers.
    * Authentication must be performed via a query parameter: `.../ws?token={jwt_token}`.
    * A new dependency, `get_current_user_from_token(token: str = Query(...))`, must be created. It will use the same logic as `deps.get_current_active_user` but read the token from the query string.
    * This dependency **must** be in the `Depends()` for the `/ws` endpoint. Unauthenticated connections must be rejected.
* **Data Ingestion Security:**
    * All data from MQTT is "untrusted" (even from an authenticated device).
    * **All** payloads *must* be parsed and validated by Pydantic schemas (`TrialResultCreate`, `DeviceLogCreate`).
    * This prevents malformed data from causing a 500 error in the ingestion handler or (worse) a SQL injection. The Pydantic validation is the primary security boundary.
* **Authorization (REST):**
    * All new REST endpoints (`.../results`, `.../logs`, `.../export`) must check ownership.
    * The user requesting data for `deployment_id` must be the same user who *created* the experiment associated with that deployment. This requires a DB join to check `deployment -> experiment -> created_by`.

---

### 10. DevOps and Infrastructure

#### 10.1 Environment Setup
* **`backend/requirements.txt`:** No major changes, `websockets` is likely already included by `uvicorn[standard]`.
* **`frontend/package.json`:** Add `"react-use-websocket": "^3.x.x"`.
* **`edge/setup.sh` (Modified):**
    * Must now also copy `edge/lics_publisher.py` to `/opt/lics-agent/` (or add it to the Python path).
    * `sudo cp ~/lics/edge/lics_publisher.py /opt/lics-agent/`
* **Action:** Developers must run `npm install` in `frontend/` and `docker-compose exec backend alembic upgrade head`.

#### 10.2 CI/CD Pipeline
* The existing pipeline will cover backend unit tests.
* A new frontend test job should be added to run `npm test` if any tests are written for the `useWebSocket` hook or UI.

#### 10.3 Monitoring and Logging
* **CRITICAL:** The `on_data_message` and `on_log_message` handlers *must* have verbose logging.
    * `logger.info(f"Received data for deployment {data.deployment_id}")`
    * `logger.error(f"Failed to parse data message: {e}", exc_info=True)`
    * `logger.error(f"Failed to validate data payload: {e}")`
* This is the *only* way to debug if data is not appearing.

---

### 11. Code Organization and Standards

* **New Backend Files:**
    ```
    backend/app/
    ├── models/
    │   ├── trial_result.py     <-- NEW
    │   └── device_log.py       <-- NEW
    ├── schemas/
    │   ├── trial_result.py     <-- NEW
    │   └── device_log.py       <-- NEW
    ├── crud/
    │   ├── crud_trial_result.py <-- NEW
    │   └── crud_device_log.py  <-- NEW
    ├── api/
    │   ├── routes/
    │   │   └── results.py      <-- NEW
    │   └── websocket.py        <-- NEW
    └── alembic/versions/
        └── ..._add_trial_results_...py <-- NEW
    ```
* **New Edge Files:**
    ```
    edge/
    └── lics_publisher.py       <-- NEW
    ```
* **New Frontend Files:**
    ```
    frontend/src/
    ├── app/(dashboard)/
    │   └── deployments/
    │       └── [id]/
    │           └── page.tsx      <-- NEW
    └── hooks/
        └── useWebSocket.ts     <-- NEW
    ```

---

### 12. Dependencies and Prerequisites

#### 12.1 External Dependencies
* `react-use-websocket` (NPM): For simplified WebSocket handling in React.

#### 12.2 Internal Dependencies
* Completion of all Phase 1, 2, and 3 objectives.
* A running Mosquitto MQTT broker.
* A functional `edge/agent.py`.
* A functional `psychopy/compiler.py`.

---

### 13. Risk Management

* **Technical Risk 1: WebSocket Connection Spam/Flapping**
    * **Likelihood:** Medium. A bad client (or buggy `useEffect`) could cause it to connect/disconnect rapidly.
    * **Mitigation:** The `ConnectionManager` is simple. The `useWebSocket` hook from `react-use-websocket` handles reconnect logic gracefully.
* **Technical Risk 2: Data Ingestion Flood (DDoS)**
    * **Likelihood:** Low (trusted network). A buggy script could publish 1000s of messages/sec.
    * **Mitigation:** The MQTT broker (Mosquitto) has built-in limits. The `on_message` handler processing (JSON parse, Pydantic validate, DB insert) acts as a natural (if inefficient) rate limit. This is acceptable for the lab scale.
* **Technical Risk 3: Real-time UI is "Janky" or Slow**
    * **Likelihood:** Medium. Broadcasting to all clients on *every* log message could be slow.
    * **Mitigation:**
        1.  *Don't.* Only broadcast `new_trial` and `deployment_status` updates.
        2.  *De-scope:* Do not broadcast `new_log` messages. Logs are less critical and can be fetched via REST.
        3.  **Refinement:** The `ConnectionManager` should be *smarter*. It should track `active_connections[user_id] = [list_of_websockets]`. The `broadcast` should be `broadcast_to_user(user_id, message)`. The MQTT handlers must look up the `user_id` from the `deployment_id` *before* broadcasting, ensuring only the data owner gets the update. This is more complex but *much* more scalable and secure.
    * **Decision:** Implement the "smarter" `ConnectionManager` (Risk 3, Mitigation 3). This is the correct architecture.

---

### 14. Phase Completion Checklist

* [ ] Alembic migration for `trial_results` and `device_logs` is created and applied.
* [ ] Backend subscribes to `.../data` and `.../logs` topics.
* [ ] Publishing to `.../data` creates a `TrialResult` record.
* [ ] Publishing to `.../logs` (from `agent.py`) creates a `DeviceLog` record.
* [ ] `GET .../results` and `GET .../logs` endpoints return correct data.
* [ ] `GET .../export/csv` endpoint returns a valid, flattened CSV file.
* [ ] Client can connect to `/ws` with a valid token.
* [ ] An MQTT message (status, data, or log) triggers a push to the correct WebSocket client.
* [ ] The `/deployments/[id]` page loads and displays historical data.
* [ ] The `/deployments/[id]` page *live-updates* when a new trial message is received.
* [ ] The `/deployments` page *live-updates* when a deployment status message is received.

---

### 15. Known Issues and Technical Debt

* **Debt:** The `ConnectionManager` is in-memory. If the single backend server/container restarts, all WebSocket connections are dropped. Clients will need to automatically reconnect. This is acceptable for the project's "simplicity first" principle. A production-grade system would use Redis Pub/Sub to manage this.
* **Issue:** The CSV export "flattens" JSON, but what if the JSON structure is inconsistent between trials?
    * **Solution:** The export function must first scan *all* trial results, build a *union* of all keys in `custom_data` and `response`, and then use that "master list" as the header, filling in `null` values where keys don't exist. This is more complex but necessary.
* **Debt:** We are modifying Phase 2 and Phase 3 artifacts (`compiler.py`, `agent.py`). This must be communicated clearly and documented in those phases as a "Phase 4 Update."

---

### 16. Lessons Learned and Retrospective

*(To be filled out upon completion of Phase 4)*

* What went well:
* What could be improved:
* Process adjustments for next phase:

---

### 17. Handoff to Next Phase

* **To: Phase 5 (Polish, Analytics & Launch) Team**
* **What is provided:**
    * A **feature-complete** LICS application.
    * The full data loop is closed: Create -> Compile -> Deploy -> Run -> Ingest -> Visualize.
    * All core database tables are implemented and populated.
    * A real-time UI provides live feedback on running experiments.
    * A data export feature provides researchers with the raw data for analysis.
* **Your (Phase 5) Objective:**
    * The system *works*, but it is not *polished*.
    * **Bug Fixing & Hardening:** Conduct thorough E2E testing and fix the (many) bugs found.
    * **UI/UX Polish:** The UI is functional but likely unrefined. Improve layouts, add loading spinners, enhance error messages, and ensure a consistent design.
    * **Documentation:** Create user-facing documentation (how to build an experiment, how to provision a Pi).
    * **Analytics (Optional):** Now that `trial_results` exists, build a new dashboard page (`/analytics`) that uses a charting library (e.g., Recharts) to plot "Reaction Time over Time," "Percent Correct by Condition," etc.
    * **User Training:** Train the lab researchers on how to use the system.